# 爬虫中遇到的一些问题

## 异步中的异常处理

第一次写的爬虫主逻辑是根据500lines中的asyncio crawler改过来的，主要是靠aiohttp这个库来实现异步访问，然而在实际运行中经常会无故卡住，而最终发现就是因为有异常被抛出，但是没被正确捕捉，而导致主事件循环loop卡住。现在还没有找到很好地往最上层抛出的方法，只能逐层去猜是哪一个地方有异常抛出。

## 网页编码问题

仍有一些网页并不是UTF-8的编码方式，所以会导致lxml分析页面时，出现中文乱码的情况。

简单拿requests来讲，requests拿到的response的content是bytes形式，是可以直接传给lxml.etree.HTML()进行分析的。

而requests.text返回的是解码后的内容，而这个解码方式是网页指定的，有可能不是UFT-8，所以将其重新编码就可以了，也就是lxml.etree.HTML(response.text.encode(response.encoding))。但是这种方法也有一定问题，就是遇到一些Unicode也不兼容的时候，分析会出错。

```python
url = 'http://op.hanhande.com/mh/'
HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; rv:51.0) Gecko/20100101 Firefox/51.0 '}

# 直接分析
response = requests.get(url, headers=HEADERS)
body = etree.HTML(response.content)
us = body.xpath('//div[@class="mh_list_li"]/ul/li/a/@href')
ts = body.xpath('//div[@class="mh_list_li"]/ul/li/a/text()')
print(len(us), len(ts))
# 保存后分析
with codecs.open('body.html', 'w', encoding = response.encoding) as f:
    f.write(response.content.decode(response.encoding))
with codecs.open('body.html', 'r') as f:
    body = f.read()
    body = etree.HTML(body)
    us = body.xpath('//div[@class="mh_list_li"]/ul/li/a/@href')
    ts = body.xpath('//div[@class="mh_list_li"]/ul/li/a/text()')
    print(len(us), len(ts))
    
    
# 运行结果为
14 14
582 582
```

为什么会这样呢，就是在解码的时候，有部分字节不能正确转换，所以最终分析的文本有问题。

个人在decode函数传入`error='ignore'`关键字来解决这个问题。

## 文件非法命名的问题

在windows下如 ？\ * | “ < > : /；都是非法字符，不允许在文件名中出现，按正常来讲，一旦出现这种情况，应该会有异常抛出，但是在使用pycharm的时候，这个异常并没有被发现，而直接在cmd下运行python就会有异常抛出。

解决方法：

就是在创建文件时，对文件名进行判断，如果有非法字符就替换调，如：str.replace('?', '')

## 请求超时的问题

```python
try:
    response = await self.session.get(
        url, allow_redirects = False, headers=HEADERS
    )
    break
except aiohttp.ClientError as client_error:
    exception = client_error
```

这是500lines 里 crawl 的连接部分的一小段代码，这个异常捕捉的确是可以捕捉到所有的连接异常，但是在实际运行中，超时异常并没有被捕捉到，不知道是否因为是windows的锅，但是后来再加上 except asyncio.TimeoutError 就可以捕捉到超时异常了。

ps: 其实在看了源码之后，aiohttp.TimeoutError也是继承了 asyncio.TimeoutError的，那为什么aiohttp的就不能被捕捉到呢，待解决。